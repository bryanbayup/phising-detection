{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMBjAbFwPePeQrdgfWpjgl8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/phising-detection/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    TFBertForPreTraining,\n",
        "    create_optimizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForMaskedLM\n",
        ")\n",
        "from tensorflow.keras.utils import Sequence"
      ],
      "metadata": {
        "id": "YHdUtgCx5TXO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationDataset(Sequence):\n",
        "    \"\"\"\n",
        "    Contoh class Dataset untuk meload data percakapan.\n",
        "    Anda harus menyesuaikan:\n",
        "    - Path dataset\n",
        "    - Cara mengambil utterance, melakukan masking,\n",
        "      dan menghasilkan label NSP.\n",
        "    - Mengolah konteks multi-turn.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path,\n",
        "                 tokenizer,\n",
        "                 max_len=128,\n",
        "                 batch_size=16,\n",
        "                 mlm_probability=0.15,\n",
        "                 nsp_ratio=0.5):\n",
        "        self.data_path = data_path\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.batch_size = batch_size\n",
        "        self.mlm_probability = mlm_probability\n",
        "        self.nsp_ratio = nsp_ratio\n",
        "\n",
        "        # Load data dari JSON\n",
        "        with open(self.data_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        # Expecting self.data to be a list of conversations\n",
        "        # Each conversation: { \"conversation_id\": ..., \"turns\": [...] }\n",
        "\n",
        "        self.samples = self.create_samples(self.data)\n",
        "\n",
        "    def create_samples(self, data):\n",
        "        # Contoh pembuatan sampel:\n",
        "        # Kita akan mengambil setiap turn dan memasangkannya dengan turn berikutnya\n",
        "        # untuk keperluan NSP. Untuk MLM, kita akan mask input_ids.\n",
        "        samples = []\n",
        "        for conv in data:\n",
        "            turns = conv['turns']\n",
        "            for i in range(len(turns)-1):\n",
        "                current_utt = turns[i]['utterance']\n",
        "                next_utt = turns[i+1]['utterance']\n",
        "\n",
        "                # Label NSP: 1 jika next_utt memang kelanjutan, 0 jika kita ambil kalimat random\n",
        "                # (contoh sederhana: dengan probabilitas nsp_ratio kita gunakan next_utt benar,\n",
        "                # sisanya gunakan utterance dari percakapan lain).\n",
        "                if np.random.rand() < self.nsp_ratio:\n",
        "                    # next_utt sesuai urutan -> NSP = 1\n",
        "                    is_next = 1\n",
        "                else:\n",
        "                    # next_utt acak dari tempat lain -> NSP = 0\n",
        "                    random_conv = np.random.choice(data)\n",
        "                    random_turn = np.random.choice(random_conv['turns'])\n",
        "                    next_utt = random_turn['utterance']\n",
        "                    is_next = 0\n",
        "\n",
        "                # Gabungkan current_utt [SEP] next_utt sesuai format NSP BERT\n",
        "                encoded = self.tokenizer.encode_plus(\n",
        "                    current_utt,\n",
        "                    next_utt,\n",
        "                    max_length=self.max_len,\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                    return_tensors='np'\n",
        "                )\n",
        "\n",
        "                input_ids = encoded['input_ids'][0]\n",
        "                attention_mask = encoded['attention_mask'][0]\n",
        "                token_type_ids = encoded['token_type_ids'][0]\n",
        "\n",
        "                # Buat label MLM\n",
        "                input_ids_masked, mlm_labels = self.mask_tokens(input_ids)\n",
        "\n",
        "                # Di sini kita belum memasukkan konteks dialog yang kompleks.\n",
        "                # Misalnya, kita dapat mengambil beberapa turn sebelumnya untuk dialog_context.\n",
        "                # Untuk sederhana, gunakan vektor nol sebagai konteks.\n",
        "                dialog_context = np.zeros((768,), dtype=np.float32)  # Dummy context\n",
        "\n",
        "                samples.append({\n",
        "                    'input_ids': input_ids_masked,\n",
        "                    'attention_mask': attention_mask,\n",
        "                    'token_type_ids': token_type_ids,\n",
        "                    'mlm_labels': mlm_labels,\n",
        "                    'nsp_label': is_next,\n",
        "                    'dialog_context': dialog_context\n",
        "                })\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def mask_tokens(self, input_ids):\n",
        "        # Masking MLM secara sederhana:\n",
        "        # 1. Tentukan indeks mana yang akan di-mask.\n",
        "        # 2. Ganti dengan [MASK] (id=103), atau random token.\n",
        "        # Pastikan untuk tidak memask token khusus (CLS, SEP).\n",
        "        # Ini adalah simplifikasi.\n",
        "        input_ids = input_ids.copy()\n",
        "        mlm_labels = np.full_like(input_ids, -100)\n",
        "\n",
        "        # Cari indeks token yang bisa di-mask (bukan [CLS]=101 dan bukan [SEP]=102, bukan padding=0)\n",
        "        special_ids = {101, 102, 0}\n",
        "        candidate_positions = [i for i, token_id in enumerate(input_ids) if token_id not in special_ids]\n",
        "\n",
        "        num_to_mask = max(1, int(len(candidate_positions)*self.mlm_probability))\n",
        "        mask_positions = np.random.choice(candidate_positions, num_to_mask, replace=False)\n",
        "\n",
        "        for pos in mask_positions:\n",
        "            mlm_labels[pos] = input_ids[pos]\n",
        "            # 80% ganti dengan [MASK]\n",
        "            # 10% ganti dengan token random\n",
        "            # 10% biarkan token asli\n",
        "            rand = np.random.rand()\n",
        "            if rand < 0.8:\n",
        "                input_ids[pos] = 103  # [MASK]\n",
        "            elif rand < 0.9:\n",
        "                input_ids[pos] = np.random.randint(999, 30000)  # random token id\n",
        "            else:\n",
        "                # dibiarkan sama\n",
        "                pass\n",
        "\n",
        "        return input_ids, mlm_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.samples)/self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch = self.samples[idx*self.batch_size: (idx+1)*self.batch_size]\n",
        "        max_len = self.max_len\n",
        "\n",
        "        input_ids = np.array([s['input_ids'] for s in batch], dtype=np.int32)\n",
        "        attention_mask = np.array([s['attention_mask'] for s in batch], dtype=np.int32)\n",
        "        token_type_ids = np.array([s['token_type_ids'] for s in batch], dtype=np.int32)\n",
        "        mlm_labels = np.array([s['mlm_labels'] for s in batch], dtype=np.int32)\n",
        "        nsp_labels = np.array([s['nsp_label'] for s in batch], dtype=np.int32)\n",
        "        dialog_context = np.array([s['dialog_context'] for s in batch], dtype=np.float32)\n",
        "\n",
        "        # Output untuk pretraining BERT adalah: (logits_mlm, logits_nsp)\n",
        "        # Kita akan menyusun label sesuai kebutuhan:\n",
        "        # Biasanya TFBertForPreTraining menggunakan label:\n",
        "        # {'labels': mlm_labels, 'next_sentence_label': nsp_labels}\n",
        "        return (input_ids, attention_mask, token_type_ids, dialog_context), {'labels': mlm_labels, 'next_sentence_label': nsp_labels}"
      ],
      "metadata": {
        "id": "9-_JjgEv5T2S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer & model backbone\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cahya/bert-base-indonesian-522M\")\n",
        "base_model = AutoModelForMaskedLM.from_pretrained(\"cahya/bert-base-indonesian-522M\")\n",
        "\n",
        "# Kita perlu membungkus model ini agar dapat memasukkan dialog_context\n",
        "class CustomDialogModel(tf.keras.Model):\n",
        "    def __init__(self, base_model):\n",
        "        super(CustomDialogModel, self).__init__()\n",
        "        self.bert_pretrain = base_model\n",
        "        # Dense untuk memproses dialog_context\n",
        "        self.context_dense = tf.keras.layers.Dense(768, activation='relu')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        input_ids, attention_mask, token_type_ids, dialog_context = inputs\n",
        "\n",
        "        # Pass to BERT\n",
        "        outputs = self.bert_pretrain.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            training=training\n",
        "        )\n",
        "        sequence_output = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
        "        pooled_output = outputs.pooler_output        # [batch, hidden_size]\n",
        "\n",
        "        # Proses dialog context, gabung ke pooled_output (contoh sederhana)\n",
        "        dialog_ctx_emb = self.context_dense(dialog_context)  # [batch, hidden_size]\n",
        "        # Gabung dengan pooled_output\n",
        "        combined_pooled = pooled_output + dialog_ctx_emb  # [batch, hidden_size]\n",
        "\n",
        "        # Gunakan head MLM & NSP dari model asli:\n",
        "        prediction_scores, seq_relationship_score = self.bert_pretrain.cls(\n",
        "            sequence_output,\n",
        "            combined_pooled,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "# Define loss dan train_step custom, karena kita punya dua label: MLM & NSP\n",
        "class CustomTrainer(tf.keras.Model):\n",
        "    def __init__(self, dialog_model):\n",
        "        super(CustomTrainer, self).__init__()\n",
        "        self.dialog_model = dialog_model\n",
        "        self.loss_fct_mlm = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "        self.loss_fct_nsp = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Di sini hanya forward pass tanpa label\n",
        "        (input_ids, attention_mask, token_type_ids, dialog_context) = inputs\n",
        "        prediction_scores, seq_relationship_score = self.dialog_model(\n",
        "            (input_ids, attention_mask, token_type_ids, dialog_context),\n",
        "            training=training\n",
        "        )\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # data akan berupa (X, y)\n",
        "        (input_ids, attention_mask, token_type_ids, dialog_context), labels = data\n",
        "        mlm_labels = labels['labels']\n",
        "        nsp_labels = labels['next_sentence_label']\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            prediction_scores, seq_relationship_score = self(\n",
        "                (input_ids, attention_mask, token_type_ids, dialog_context),\n",
        "                training=True\n",
        "            )\n",
        "            # Hitung MLM Loss\n",
        "            mlm_active_loss = tf.not_equal(mlm_labels, -100)\n",
        "            mlm_loss = self.loss_fct_mlm(mlm_labels, prediction_scores)\n",
        "            mlm_loss = (tf.reduce_sum(mlm_loss * tf.cast(mlm_active_loss, dtype=mlm_loss.dtype)) /\n",
        "                         (tf.reduce_sum(tf.cast(mlm_active_loss, tf.float32)) + 1e-5))\n",
        "\n",
        "            # Hitung NSP Loss\n",
        "            nsp_loss = self.loss_fct_nsp(nsp_labels, seq_relationship_score)\n",
        "            nsp_loss = tf.reduce_mean(nsp_loss)\n",
        "\n",
        "            total_loss = mlm_loss + nsp_loss\n",
        "\n",
        "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        # log metrics\n",
        "        self.compiled_metrics.update_state(total_loss)\n",
        "        return {\"loss\": total_loss, \"mlm_loss\": mlm_loss, \"nsp_loss\": nsp_loss}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        (input_ids, attention_mask, token_type_ids, dialog_context), labels = data\n",
        "        mlm_labels = labels['labels']\n",
        "        nsp_labels = labels['next_sentence_label']\n",
        "\n",
        "        prediction_scores, seq_relationship_score = self(\n",
        "            (input_ids, attention_mask, token_type_ids, dialog_context),\n",
        "            training=False\n",
        "        )\n",
        "        # Hitung MLM Loss\n",
        "        mlm_active_loss = tf.not_equal(mlm_labels, -100)\n",
        "        mlm_loss = self.loss_fct_mlm(mlm_labels, prediction_scores)\n",
        "        mlm_loss = (tf.reduce_sum(mlm_loss * tf.cast(mlm_active_loss, dtype=mlm_loss.dtype)) /\n",
        "                     (tf.reduce_sum(tf.cast(mlm_active_loss, tf.float32)) + 1e-5))\n",
        "\n",
        "        # Hitung NSP Loss\n",
        "        nsp_loss = self.loss_fct_nsp(nsp_labels, seq_relationship_score)\n",
        "        nsp_loss = tf.reduce_mean(nsp_loss)\n",
        "\n",
        "        total_loss = mlm_loss + nsp_loss\n",
        "        return {\"loss\": total_loss, \"mlm_loss\": mlm_loss, \"nsp_loss\": nsp_loss}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksNJfi3s5dvu",
        "outputId": "bffd07e7-01be-40f3-beed-770866c6e009"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cahya/bert-base-indonesian-522M were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"data2.json\"\n",
        "train_dataset = ConversationDataset(data_path=data_path, tokenizer=tokenizer, max_len=128, batch_size=8)\n",
        "val_dataset = ConversationDataset(data_path=data_path, tokenizer=tokenizer, max_len=128, batch_size=8)  # Contoh sama, seharusnya beda data\n",
        "\n",
        "train_steps = len(train_dataset)\n",
        "val_steps = len(val_dataset)\n",
        "\n",
        "num_epochs = 2\n",
        "batch_size = 8\n",
        "initial_lr = 3e-5\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
        "\n",
        "dialog_model = CustomDialogModel(base_model)\n",
        "trainer_model = CustomTrainer(dialog_model)\n",
        "trainer_model.compile(optimizer=optimizer)\n",
        "\n",
        "history = trainer_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=num_epochs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "SySZRkB-5jHB",
        "outputId": "384decee-b846-4a41-cb94-94056a827d8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:1383: UserWarning: Layer 'custom_dialog_model_2' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''BertModel.forward() got an unexpected keyword argument 'training'''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'custom_dialog_model_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:1383: UserWarning: Layer 'custom_trainer_2' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''Exception encountered when calling CustomDialogModel.call().\n",
            "\n",
            "\u001b[1mBertModel.forward() got an unexpected keyword argument 'training'\u001b[0m\n",
            "\n",
            "Arguments received by CustomDialogModel.call():\n",
            "  • inputs=('tf.Tensor(shape=(None, 128), dtype=int32)', 'tf.Tensor(shape=(None, 128), dtype=int32)', 'tf.Tensor(shape=(None, 128), dtype=int32)', 'tf.Tensor(shape=(None, 768), dtype=float32)')\n",
            "  • training=True''\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Exception encountered when calling CustomDialogModel.call().\n\n\u001b[1mBertModel.forward() got an unexpected keyword argument 'training'\u001b[0m\n\nArguments received by CustomDialogModel.call():\n  • inputs=('tf.Tensor(shape=(None, 128), dtype=int32)', 'tf.Tensor(shape=(None, 128), dtype=int32)', 'tf.Tensor(shape=(None, 128), dtype=int32)', 'tf.Tensor(shape=(None, 768), dtype=float32)')\n  • training=True",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ef38de834458>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtrainer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m history = trainer_model.fit(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d76844f1fd7e>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             prediction_scores, seq_relationship_score = self(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialog_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d76844f1fd7e>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Di sini hanya forward pass tanpa label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialog_context\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         prediction_scores, seq_relationship_score = self.dialog_model(\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialog_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d76844f1fd7e>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Pass to BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         outputs = self.bert_pretrain.bert(\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling CustomDialogModel.call().\n\n\u001b[1mBertModel.forward() got an unexpected keyword argument 'training'\u001b[0m\n\nArguments received by CustomDialogModel.call():\n  • inputs=('tf.Tensor(shape=(None, 128), dtype=int32)', 'tf.Tensor(shape=(None, 128), dtype=int32)', 'tf.Tensor(shape=(None, 128), dtype=int32)', 'tf.Tensor(shape=(None, 768), dtype=float32)')\n  • training=True"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M1QgTgHi6C6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}