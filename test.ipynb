{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLnpmM9aKFQdrAXvJWRbre",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e6d53414a29a4ca89ff455b3caab2322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f938e5a52fbd433b8e70cd49c1db33cf",
              "IPY_MODEL_abf40e6e99814cf7b461796e9af7590d",
              "IPY_MODEL_cbd4dbf33dc042adb9bd4523d400939d"
            ],
            "layout": "IPY_MODEL_cca49e1c484c4812a091190f3780b60f"
          }
        },
        "f938e5a52fbd433b8e70cd49c1db33cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83f0b0c2fea646839d3b06ae67aff2fe",
            "placeholder": "​",
            "style": "IPY_MODEL_e7b8dec2fd7548f893681d9d31020efe",
            "value": "Map: 100%"
          }
        },
        "abf40e6e99814cf7b461796e9af7590d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_628af89cbf70497a88e9f64c3608a24a",
            "max": 387,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6d0511d266e4439aa113dc5f4b0fd0c",
            "value": 387
          }
        },
        "cbd4dbf33dc042adb9bd4523d400939d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4ca7c3e1c0a453dbff4442635a2d96f",
            "placeholder": "​",
            "style": "IPY_MODEL_20a3f6ffdeed42d18daf21c770606f02",
            "value": " 387/387 [00:00&lt;00:00, 3170.79 examples/s]"
          }
        },
        "cca49e1c484c4812a091190f3780b60f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f0b0c2fea646839d3b06ae67aff2fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7b8dec2fd7548f893681d9d31020efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "628af89cbf70497a88e9f64c3608a24a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6d0511d266e4439aa113dc5f4b0fd0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4ca7c3e1c0a453dbff4442635a2d96f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20a3f6ffdeed42d18daf21c770606f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/phising-detection/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip allinone.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHvqaNDplmsR",
        "outputId": "b8f99b8c-3861-487f-b2f3-d1f135a58714"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  allinone.zip\n",
            "   creating: app/\n",
            "   creating: app/models/\n",
            "  inflating: app/models/model_intent.keras  \n",
            "  inflating: app/models/model_ner.keras  \n",
            "   creating: app/data/\n",
            "  inflating: app/data/vectorizer.pickle  \n",
            "  inflating: app/data/stopword_list_tala.txt  \n",
            "  inflating: app/data/dataaa.json    \n",
            "   creating: app/encoders/\n",
            "  inflating: app/encoders/tokenizer.pickle  \n",
            "  inflating: app/encoders/ner_label_encoder.pickle  \n",
            "  inflating: app/encoders/label_encoder.pickle  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Bidirectional, LSTM, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TFAutoModelForCausalLM,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "import nltk"
      ],
      "metadata": {
        "id": "6ZWfziN6kyoB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7XQ0xyHk01D",
        "outputId": "fdfe371b-3e4e-414f-cb51-18c2a16f91bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/app')\n",
        "os.makedirs('data', exist_ok=True)\n",
        "data_path = 'data/data2.json'\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Ekstrak user turns untuk intent & NER\n",
        "utterances = []\n",
        "intents = []\n",
        "entities_all = []\n",
        "\n",
        "for conv in data:\n",
        "    for turn in conv['turns']:\n",
        "        if turn['speaker'] == 'user':\n",
        "            utt = turn['utterance']\n",
        "            intent = turn['intent']\n",
        "            ents = turn.get('entities', [])\n",
        "            utterances.append(utt)\n",
        "            intents.append(intent)\n",
        "            entities_all.append(ents)\n",
        "\n",
        "df = pd.DataFrame({'utterance': utterances, 'intent': intents, 'entities': entities_all})\n",
        "\n",
        "print(\"Contoh data intent & NER:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ9-I-Ihk49m",
        "outputId": "c52f092b-197f-4bce-bc6f-c3131a87eebc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contoh data intent & NER:\n",
            "                                           utterance  \\\n",
            "0    Saya melihat seekor kucing sakit di depan toko.   \n",
            "1           Kucing terlihat demam dan bersin-bersin.   \n",
            "2                  Ada anjing terluka di jalan raya.   \n",
            "3              Ya, anjing muntah dan terlihat lemas.   \n",
            "4  Saya menemukan seekor kucing dengan luka di ka...   \n",
            "\n",
            "                       intent  \\\n",
            "0  Melaporkan Hewan Terlantar   \n",
            "1         Mendiagnosis Gejala   \n",
            "2  Melaporkan Hewan Terlantar   \n",
            "3         Mendiagnosis Gejala   \n",
            "4  Melaporkan Hewan Terlantar   \n",
            "\n",
            "                                            entities  \n",
            "0  [{'entity': 'animal', 'value': 'kucing', 'star...  \n",
            "1  [{'entity': 'animal', 'value': 'kucing', 'star...  \n",
            "2  [{'entity': 'animal', 'value': 'anjing', 'star...  \n",
            "3  [{'entity': 'animal', 'value': 'anjing', 'star...  \n",
            "4  [{'entity': 'animal', 'value': 'kucing', 'star...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Jika perlu stopwords dan stemming (disesuaikan dengan kebutuhan)\n",
        "# Contoh tanpa stopwords dan stemming untuk kesederhanaan\n",
        "df['utterance_clean'] = df['utterance'].apply(clean_text)"
      ],
      "metadata": {
        "id": "dOthYAoOmKzE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install tree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2j_Yj33m313",
        "outputId": "7d9f33b5-a8d0-46ce-9862-cc9f426af687"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 0s (121 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "df['intent_label'] = label_encoder.fit_transform(df['intent'])\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Create the 'encoders' directory first\n",
        "os.makedirs('encoders', exist_ok=True)\n",
        "\n",
        "# Then, open a file *within* the directory for writing\n",
        "with open('encoders/label_encoder_new.pickle', 'wb') as f:  # Specify a filename within the directory\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "texts = df['utterance_clean'].tolist()\n",
        "labels = df['intent_label'].tolist()\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Tokenizer untuk intent\n",
        "tokenizer_intent = tf.keras.preprocessing.text.Tokenizer(oov_token='')\n",
        "tokenizer_intent.fit_on_texts(train_texts)\n",
        "word_index = tokenizer_intent.word_index\n",
        "vocab_size = len(word_index)+1\n",
        "\n",
        "train_seq = tokenizer_intent.texts_to_sequences(train_texts)\n",
        "val_seq = tokenizer_intent.texts_to_sequences(val_texts)\n",
        "\n",
        "max_seq_length = max(max(len(s) for s in train_seq), max(len(s) for s in val_seq))\n",
        "train_padded = pad_sequences(train_seq, maxlen=max_seq_length, padding='post')\n",
        "val_padded = pad_sequences(val_seq, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "train_labels_cat = to_categorical(train_labels, num_classes=num_classes)\n",
        "val_labels_cat = to_categorical(val_labels, num_classes=num_classes)\n",
        "\n",
        "# Embedding Sederhana (Random), Anda bisa load FastText jika ada\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.random.normal(scale=0.6, size=(vocab_size, embedding_dim))\n",
        "\n",
        "with open('encoders/tokenizer_intent_new.pickle', 'wb') as f:\n",
        "    pickle.dump(tokenizer_intent, f)"
      ],
      "metadata": {
        "id": "z-eaYGQ4mntG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_intent_model(embedding_matrix, max_seq_length, num_classes, l2_reg=0.001):\n",
        "    inputs = Input(shape=(max_seq_length,))\n",
        "    embedding = Embedding(\n",
        "        input_dim=embedding_matrix.shape[0],\n",
        "        output_dim=embedding_matrix.shape[1],\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_seq_length,\n",
        "        trainable=True\n",
        "    )(inputs)\n",
        "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedding)\n",
        "    pool = GlobalMaxPooling1D()(conv)\n",
        "    dense = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(pool)\n",
        "    drop = Dropout(0.5)(dense)\n",
        "    outputs = Dense(num_classes, activation='softmax')(drop)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "model_intent = build_intent_model(embedding_matrix, max_seq_length, num_classes)\n",
        "model_intent.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_intent.fit(\n",
        "    train_padded, train_labels_cat,\n",
        "    validation_data=(val_padded, val_labels_cat),\n",
        "    epochs=5,\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "model_intent.save('models/model_intent_new.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOu4cxmSmryu",
        "outputId": "5fd88048-689c-41d7-df0d-9af501222016"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 393ms/step - accuracy: 0.2323 - loss: 2.1279 - val_accuracy: 0.5000 - val_loss: 1.0290\n",
            "Epoch 2/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6472 - loss: 0.9639 - val_accuracy: 0.7000 - val_loss: 0.7364\n",
            "Epoch 3/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8151 - loss: 0.5949 - val_accuracy: 0.7667 - val_loss: 0.5978\n",
            "Epoch 4/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9163 - loss: 0.4270 - val_accuracy: 0.8333 - val_loss: 0.4976\n",
            "Epoch 5/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9633 - loss: 0.2958 - val_accuracy: 0.8333 - val_loss: 0.4418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat label NER\n",
        "all_labels = set(['O'])\n",
        "for ents in df['entities']:\n",
        "    for ent in ents:\n",
        "        ent_type = ent['entity']\n",
        "        all_labels.add('B-' + ent_type)\n",
        "        all_labels.add('I-' + ent_type)\n",
        "\n",
        "ner_label_encoder = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
        "ner_label_decoder = {idx: label for label, idx in ner_label_encoder.items()}\n",
        "\n",
        "with open('encoders/ner_label_encoder_new.pickle', 'wb') as f:\n",
        "    pickle.dump(ner_label_encoder, f)\n",
        "\n",
        "def create_ner_data(texts, entities_list, tokenizer, max_len):\n",
        "    X = []\n",
        "    Y = []\n",
        "    for text, ents in zip(texts, entities_list):\n",
        "        words = text.split()\n",
        "        seq = tokenizer.texts_to_sequences([text])[0]\n",
        "        label_seq = ['O'] * len(seq)\n",
        "\n",
        "        # Pemetaan sederhana entitas -> token (berdasarkan substring match)\n",
        "        # Jika sistem tokenisasi lebih rumit, perbaiki logika ini.\n",
        "        for ent in ents:\n",
        "            ent_text = clean_text(ent['value'])\n",
        "            ent_tokens = ent_text.split()\n",
        "            for i in range(len(words)-len(ent_tokens)+1):\n",
        "                if words[i:i+len(ent_tokens)] == ent_tokens:\n",
        "                    label_seq[i] = 'B-' + ent['entity']\n",
        "                    for j in range(1, len(ent_tokens)):\n",
        "                        label_seq[i+j] = 'I-' + ent['entity']\n",
        "                    break\n",
        "\n",
        "        # Padding\n",
        "        if len(seq) < max_len:\n",
        "            seq += [0]*(max_len-len(seq))\n",
        "            label_seq += ['O']*(max_len-len(label_seq))\n",
        "        else:\n",
        "            seq = seq[:max_len]\n",
        "            label_seq = label_seq[:max_len]\n",
        "\n",
        "        X.append(seq)\n",
        "        Y.append([ner_label_encoder[l] for l in label_seq])\n",
        "\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "    Y = to_categorical(Y, num_classes=len(ner_label_encoder))\n",
        "    return X, Y\n",
        "\n",
        "X_ner, Y_ner = create_ner_data(df['utterance_clean'].tolist(), df['entities'].tolist(), tokenizer_intent, max_seq_length)\n",
        "X_ner_train, X_ner_val, Y_ner_train, Y_ner_val = train_test_split(X_ner, Y_ner, test_size=0.2, random_state=42)\n",
        "\n",
        "def build_ner_model(embedding_matrix, max_seq_length, num_entities):\n",
        "    inputs = Input(shape=(max_seq_length,))\n",
        "    embedding = Embedding(\n",
        "        input_dim=embedding_matrix.shape[0],\n",
        "        output_dim=embedding_matrix.shape[1],\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_seq_length,\n",
        "        trainable=True\n",
        "    )(inputs)\n",
        "    lstm = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n",
        "    drop = Dropout(0.5)(lstm)\n",
        "    outputs = TimeDistributed(Dense(num_entities, activation='softmax'))(drop)\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_ner = build_ner_model(embedding_matrix, max_seq_length, len(ner_label_encoder))\n",
        "model_ner.fit(\n",
        "    X_ner_train, Y_ner_train,\n",
        "    validation_data=(X_ner_val, Y_ner_val),\n",
        "    epochs=5,\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "model_ner.save('models/model_ner_new.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjVVZ1nOn-mx",
        "outputId": "f2c03a2f-1b84-43f7-c5c1-c547ee32d2c3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - accuracy: 0.5959 - loss: 1.4359 - val_accuracy: 0.8146 - val_loss: 0.7039\n",
            "Epoch 2/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8608 - loss: 0.5364 - val_accuracy: 0.8333 - val_loss: 0.5623\n",
            "Epoch 3/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8846 - loss: 0.4165 - val_accuracy: 0.8771 - val_loss: 0.4630\n",
            "Epoch 4/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9122 - loss: 0.3296 - val_accuracy: 0.8792 - val_loss: 0.3932\n",
            "Epoch 5/5\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9207 - loss: 0.2726 - val_accuracy: 0.8833 - val_loss: 0.3407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSUGgHagovEe",
        "outputId": "6ad420a5-fc62-4e48-b045-cd2878c9be3e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat corpus untuk Language Modeling dari dataset percakapan\n",
        "lm_texts = []\n",
        "for conv in data:\n",
        "    for turn in conv['turns']:\n",
        "        prefix = \"User:\" if turn['speaker'] == 'user' else \"Bot:\"\n",
        "        lm_texts.append(f\"{prefix} {turn['utterance']}\")\n",
        "    lm_texts.append(\"\")  # Pisahkan tiap percakapan dengan newline\n",
        "\n",
        "lm_corpus = \"\\n\".join(lm_texts)\n",
        "with open('data/lm_corpus.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(lm_corpus)\n",
        "\n",
        "gpt_model_name = \"cahya/gpt2-small-indonesian-522M\"\n",
        "gpt_tokenizer = AutoTokenizer.from_pretrained(gpt_model_name)\n",
        "gpt_model = AutoModelForCausalLM.from_pretrained(gpt_model_name)\n",
        "\n",
        "# Add a padding token to the tokenizer\n",
        "if gpt_tokenizer.pad_token is None:\n",
        "    gpt_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    gpt_model.resize_token_embeddings(len(gpt_tokenizer)) # Important: Resize model embeddings\n",
        "\n",
        "# Load dataset untuk huggingface\n",
        "lines = lm_corpus.splitlines()\n",
        "dataset = Dataset.from_dict({'text': lines})\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return gpt_tokenizer(examples[\"text\"], truncation=True, max_length=128, padding='max_length')\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=gpt_tokenizer,\n",
        "    mlm=False,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt_finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    logging_dir='./logs',\n",
        "    do_train=True,\n",
        "    do_eval=False\n",
        ")\n",
        "\n",
        "# Kita hanya punya training dataset, bagi sedikit untuk eval jika perlu\n",
        "# Misalnya 90/10 split\n",
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = tokenized_dataset['train']\n",
        "eval_dataset = tokenized_dataset['test']\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=gpt_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./gpt_finetuned\")\n",
        "\n",
        "print(\"Proses updating model selesai! Model intents, NER dan GPT telah diperbarui sesuai dataset baru.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471,
          "referenced_widgets": [
            "e6d53414a29a4ca89ff455b3caab2322",
            "f938e5a52fbd433b8e70cd49c1db33cf",
            "abf40e6e99814cf7b461796e9af7590d",
            "cbd4dbf33dc042adb9bd4523d400939d",
            "cca49e1c484c4812a091190f3780b60f",
            "83f0b0c2fea646839d3b06ae67aff2fe",
            "e7b8dec2fd7548f893681d9d31020efe",
            "628af89cbf70497a88e9f64c3608a24a",
            "a6d0511d266e4439aa113dc5f4b0fd0c",
            "f4ca7c3e1c0a453dbff4442635a2d96f",
            "20a3f6ffdeed42d18daf21c770606f02"
          ]
        },
        "id": "6RSlL4iCoJbY",
        "outputId": "97db0487-27a8-4e63-e779-6b7de09531bf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/387 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6d53414a29a4ca89ff455b3caab2322"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/app/wandb/run-20241206_062718-xo816hcp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bryanazza836-universitas-siber-asia/huggingface/runs/xo816hcp' target=\"_blank\">./gpt_finetuned</a></strong> to <a href='https://wandb.ai/bryanazza836-universitas-siber-asia/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bryanazza836-universitas-siber-asia/huggingface' target=\"_blank\">https://wandb.ai/bryanazza836-universitas-siber-asia/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bryanazza836-universitas-siber-asia/huggingface/runs/xo816hcp' target=\"_blank\">https://wandb.ai/bryanazza836-universitas-siber-asia/huggingface/runs/xo816hcp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='174' max='174' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [174/174 00:32, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.150600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proses updating model selesai! Model intents, NER dan GPT telah diperbarui sesuai dataset baru.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the label encoder for intent prediction\n",
        "with open('encoders/label_encoder.pickle', 'rb') as f:\n",
        "    label_encoder_intent = pickle.load(f)"
      ],
      "metadata": {
        "id": "6R21pwpNqnSu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_intent(utterance):\n",
        "    seq = tokenizer_intent.texts_to_sequences([utterance])\n",
        "    padded_seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=model_intent.input_shape[1], padding='post')\n",
        "    predictions = model_intent.predict(padded_seq)\n",
        "    intent = label_encoder_intent.inverse_transform([predictions.argmax(axis=1)[0]])  # Use loaded label_encoder_intent\n",
        "    return intent[0]\n",
        "def predict_ner(utterance):\n",
        "    words = utterance.split()\n",
        "    seq = tokenizer_intent.texts_to_sequences([utterance])[0]\n",
        "    padded_seq = tf.keras.preprocessing.sequence.pad_sequences([seq], maxlen=model_ner.input_shape[1], padding='post')\n",
        "    predictions = model_ner.predict(padded_seq).argmax(axis=-1)[0]\n",
        "    labels = [ner_label_decoder[p] for p in predictions[:len(words)]]\n",
        "    return list(zip(words, labels))\n",
        "def generate_response(context, max_length=50):\n",
        "    input_ids = gpt_tokenizer(context, return_tensors='pt').input_ids\n",
        "    # Move input_ids to the same device as the model\n",
        "    input_ids = input_ids.to(gpt_model.device)\n",
        "    gen_tokens = gpt_model.generate(input_ids, max_length=max_length, num_return_sequences=1, temperature=0.9)\n",
        "    gen_text = gpt_tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
        "    return gen_text\n",
        "utterance = \"Saya melihat seekor kucing sakit di depan toko.\"\n",
        "intent = predict_intent(utterance)\n",
        "print(f\"Intent: {intent}\")\n",
        "ner_results = predict_ner(utterance)\n",
        "print(\"Entities:\")\n",
        "for word, label in ner_results:\n",
        "    print(f\"  {word}: {label}\")\n",
        "context = \"\"\"User: Saya melihat seekor kucing sakit di depan toko.\n",
        "Bot: Terima kasih atas laporannya. Apakah kucing tersebut menunjukkan gejala seperti demam atau muntah?\n",
        "User: Kucing terlihat muntah dan lemas.\"\"\"\n",
        "\n",
        "response = generate_response(context)\n",
        "print(f\"Bot Response: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG7Y8ThGoTnc",
        "outputId": "90be2029-98f0-439e-d76d-90b80504c0c1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Intent: Melaporkan Hewan Terlantar\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities:\n",
            "  Saya: O\n",
            "  melihat: O\n",
            "  seekor: O\n",
            "  kucing: B-animal\n",
            "  sakit: O\n",
            "  di: O\n",
            "  depan: O\n",
            "  toko.: O\n",
            "Bot Response: User: Saya melihat seekor kucing sakit di depan toko.\n",
            "Bot: Terima kasih atas laporannya. Apakah kucing tersebut menunjukkan gejala seperti demam atau muntah?\n",
            "User: Kucing terlihat muntah dan lemas. Apa yang harus dilakukan???\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BliO-gBJqRV2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}