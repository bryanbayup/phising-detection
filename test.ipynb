{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhaUWvcL6CGPGi8ermxGUs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/phising-detection/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval tensorflow==2.8.0 tensorflow-addons protobuf==3.20.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9pqtziL5__Z",
        "outputId": "e18d10bd-fe83-44c0-fd96-0a6e44e46a1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: tensorflow==2.8.0 in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: protobuf==3.20.0 in /usr/local/lib/python3.10/dist-packages (3.20.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (24.3.25)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.68.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.2)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.45.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Download FastText\n",
        "!wget -O id.tar.gz \"https://www.dropbox.com/scl/fi/sju4o3keikox69euw51vy/id.tar.gz?rlkey=5jr3ijtbdwfahq7xcgig28qvy&e=1&st=gntzkzeo&dl=1\"\n",
        "!tar -xzf id.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0h6zQcm7W34",
        "outputId": "db2fd4d9-8eac-4687-f56e-48dc0f449dce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-06 07:44:48--  https://www.dropbox.com/scl/fi/sju4o3keikox69euw51vy/id.tar.gz?rlkey=5jr3ijtbdwfahq7xcgig28qvy&e=1&st=gntzkzeo&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca3416caccb22fde9b99306f935.dl.dropboxusercontent.com/cd/0/inline/CfvzgSa_9BXK5ZFo7sfhEoW4dlcptu1FEhWiGz2M_tz_3U7PrkroThZ6heGEB9OpzY666Ysr2x-MFUuQLzSnNHaQbS7q8TcqccWwgMN1Y24V_EKOvCl-9azVCsPFlrEO0y0/file?dl=1# [following]\n",
            "--2024-12-06 07:44:49--  https://uca3416caccb22fde9b99306f935.dl.dropboxusercontent.com/cd/0/inline/CfvzgSa_9BXK5ZFo7sfhEoW4dlcptu1FEhWiGz2M_tz_3U7PrkroThZ6heGEB9OpzY666Ysr2x-MFUuQLzSnNHaQbS7q8TcqccWwgMN1Y24V_EKOvCl-9azVCsPFlrEO0y0/file?dl=1\n",
            "Resolving uca3416caccb22fde9b99306f935.dl.dropboxusercontent.com (uca3416caccb22fde9b99306f935.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uca3416caccb22fde9b99306f935.dl.dropboxusercontent.com (uca3416caccb22fde9b99306f935.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CfvejobLV2nrsowux2iLs3pcBEUIzpyg92w8zopaPxRC3DmglPGexS9CelXb0ri7El6XIEYrRnuaAQ4-M52RKSdmqmyd2yqq2B3MZBM-wZwKpRBOX8JlzdRzz5Fb4IqQQI8jK6gUQOm84BQPhArwWKhxL11oC-Mfg2FQFdLlP-yXqdpktJUdVEKULzQ-h4F9jVcb4neha7dgb56lgSlBezJMbZzZeMOv3M66xqS9C6TXqrxXgbIXE7VvJ48PuEM0U00nNY8IfaJGgeQ-8z7AKG8azqwXQrS1d6Z3XsgK_dRmR0WGvbeIoET9etf70lXPIGeXs2rzyRYSNq06r9xpXwBGDzrKr40g3metV2fQNn3Q3Q/file?dl=1 [following]\n",
            "--2024-12-06 07:44:49--  https://uca3416caccb22fde9b99306f935.dl.dropboxusercontent.com/cd/0/inline2/CfvejobLV2nrsowux2iLs3pcBEUIzpyg92w8zopaPxRC3DmglPGexS9CelXb0ri7El6XIEYrRnuaAQ4-M52RKSdmqmyd2yqq2B3MZBM-wZwKpRBOX8JlzdRzz5Fb4IqQQI8jK6gUQOm84BQPhArwWKhxL11oC-Mfg2FQFdLlP-yXqdpktJUdVEKULzQ-h4F9jVcb4neha7dgb56lgSlBezJMbZzZeMOv3M66xqS9C6TXqrxXgbIXE7VvJ48PuEM0U00nNY8IfaJGgeQ-8z7AKG8azqwXQrS1d6Z3XsgK_dRmR0WGvbeIoET9etf70lXPIGeXs2rzyRYSNq06r9xpXwBGDzrKr40g3metV2fQNn3Q3Q/file?dl=1\n",
            "Reusing existing connection to uca3416caccb22fde9b99306f935.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2333351997 (2.2G) [application/binary]\n",
            "Saving to: ‘id.tar.gz’\n",
            "\n",
            "id.tar.gz           100%[===================>]   2.17G  47.8MB/s    in 28s     \n",
            "\n",
            "2024-12-06 07:45:18 (80.2 MB/s) - ‘id.tar.gz’ saved [2333351997/2333351997]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from seqeval.metrics import classification_report as seq_classification_report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXL3brrW7CmQ",
        "outputId": "d15d6aed-388f-4143-8db2-41ce496f03a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YhKq2Z39SHR",
        "outputId": "c63b4d45-df08-46d0-c632-d2372c329e66"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "HAlFY-749P0n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Memuat stopwords\n",
        "with open('stopword_list_tala.txt', 'r', encoding='utf-8') as f:\n",
        "    stop_words = f.read().splitlines()\n",
        "stop_words = set(word.strip().lower() for word in stop_words)  # Pastikan lowercase dan tanpa spasi\n",
        "\n",
        "# Stemming dengan Sastrawi\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Fungsi preprocessing teks\n",
        "def preprocess_text(text):\n",
        "    # Konversi teks menjadi huruf kecil dan hapus tanda baca\n",
        "    text = clean_text(text)\n",
        "    # Tokenisasi\n",
        "    tokens = text.split()\n",
        "    # Penghapusan stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Stemming\n",
        "    tokens_after_stemming = [stemmer.stem(token) for token in tokens]\n",
        "    # Gabungkan kembali tokens menjadi string\n",
        "    text = ' '.join(tokens_after_stemming)\n",
        "    return text"
      ],
      "metadata": {
        "id": "AwF64ufK7GUq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_model = KeyedVectors.load_word2vec_format('id.vec', binary=False)"
      ],
      "metadata": {
        "id": "uYXKCclL9kVo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'data2.json'  # ganti dengan nama file dataset Anda\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Ekstrak user turns\n",
        "utterances = []\n",
        "intents = []\n",
        "entities_all = []\n",
        "\n",
        "for conv in data:\n",
        "    for turn in conv['turns']:\n",
        "        if turn['speaker'] == 'user':\n",
        "            utt = turn['utterance']\n",
        "            intent = turn['intent']\n",
        "            ents = turn.get('entities', [])\n",
        "            utterances.append(utt)\n",
        "            intents.append(intent)\n",
        "            entities_all.append(ents)\n",
        "\n",
        "df = pd.DataFrame({'utterance': utterances, 'intent': intents, 'entities': entities_all})\n",
        "df['utterance_clean'] = df['utterance'].apply(preprocess_text)\n",
        "\n",
        "# Label intent\n",
        "intent_labels = sorted(df['intent'].unique().tolist())\n",
        "intent_label2id = {l:i for i,l in enumerate(intent_labels)}\n",
        "intent_id2label = {i:l for l,i in intent_label2id.items()}\n",
        "df['intent_id'] = df['intent'].map(intent_label2id)\n",
        "\n",
        "texts = df['utterance_clean'].tolist()\n",
        "Y_intent = df['intent_id'].values\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='')\n",
        "tokenizer.fit_on_texts(texts)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)+1\n",
        "\n",
        "seqs = tokenizer.texts_to_sequences(texts)\n",
        "max_seq_length = max(len(s) for s in seqs)\n",
        "max_seq_length = min(max_seq_length, 60)  # tingkatkan max length jika perlu\n",
        "X_padded = pad_sequences(seqs, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# NER Labeling\n",
        "all_ner_labels = set(['O'])\n",
        "for ents in entities_all:\n",
        "    for e in ents:\n",
        "        ent_type = e['entity']\n",
        "        all_ner_labels.add(\"B-\"+ent_type)\n",
        "        all_ner_labels.add(\"I-\"+ent_type)\n",
        "all_ner_labels = sorted(list(all_ner_labels))\n",
        "ner_label2id = {l:i for i,l in enumerate(all_ner_labels)}\n",
        "ner_id2label = {i:l for l,i in ner_label2id.items()}\n",
        "\n",
        "def create_ner_labels(texts, entities_all, tokenizer, max_len):\n",
        "    all_ner_seq = []\n",
        "    for txt, ents in zip(texts, entities_all):\n",
        "        tok_seq = tokenizer.texts_to_sequences([txt])[0]\n",
        "        ner_seq = ['O']*len(tok_seq)\n",
        "        words = txt.split()\n",
        "        for e in ents:\n",
        "            ent_value = preprocess_text(e['value'])\n",
        "            ent_words = ent_value.split()\n",
        "            for i in range(len(words)-len(ent_words)+1):\n",
        "                if words[i:i+len(ent_words)] == ent_words:\n",
        "                    for w_j in range(len(ent_words)):\n",
        "                        if i+w_j < len(ner_seq):\n",
        "                            ner_seq[i+w_j] = 'B-'+e['entity'] if w_j == 0 else 'I-'+e['entity']\n",
        "                    break\n",
        "        if len(ner_seq) > max_len:\n",
        "            ner_seq = ner_seq[:max_len]\n",
        "        else:\n",
        "            ner_seq += ['O']*(max_len-len(ner_seq))\n",
        "        all_ner_seq.append(ner_seq)\n",
        "    return all_ner_seq\n",
        "\n",
        "ner_labels = create_ner_labels(df['utterance_clean'].tolist(), df['entities'].tolist(), tokenizer, max_seq_length)\n",
        "ner_ids = [[ner_label2id[l] for l in seq] for seq in ner_labels]\n",
        "Y_ner = np.array(ner_ids) # Nanti CRF pakai sparse target\n",
        "\n",
        "X_train, X_val, Y_train_intent, Y_val_intent, Y_train_ner, Y_val_ner = train_test_split(\n",
        "    X_padded, Y_intent, Y_ner, test_size=0.2, random_state=42, stratify=Y_intent\n",
        ")\n",
        "\n",
        "# Embedding\n",
        "embedding_dim = fasttext_model.vector_size\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in word_index.items():\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix[idx] = fasttext_model[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))"
      ],
      "metadata": {
        "id": "1_3FWYJk89V3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Layer\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "        self.dense = Dense(1)  # Define Dense layer here\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: (batch, seq, dim)\n",
        "        # Compute attention score\n",
        "        w = self.dense(inputs)  # (batch, seq, 1)\n",
        "        a = tf.nn.softmax(w, axis=1)  # attention weights\n",
        "        context = tf.reduce_sum(a * inputs, axis=1)  # (batch, dim)\n",
        "        return context\n",
        "\n",
        "num_intent = len(intent_label2id)\n",
        "num_ner = len(ner_label2id)\n",
        "\n",
        "input_layer = Input(shape=(max_seq_length,))\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,\n",
        "    output_dim=embedding_dim,\n",
        "    weights=[embedding_matrix],\n",
        "    input_length=max_seq_length,\n",
        "    trainable=True\n",
        ")(input_layer)\n",
        "\n",
        "x = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Attention untuk intent\n",
        "context = AttentionLayer()(x)  # Use the custom AttentionLayer\n",
        "intent_out = Dense(num_intent, activation='softmax', name='intent_output')(context)\n",
        "\n",
        "# NER dengan CRF\n",
        "ner_dense = Dense(num_ner)(x)\n",
        "# Kita butuh Custom CRF layer, tapi di tensorflow-addons sudah disediakan fungsi CRF ops.\n",
        "# Akan gunakan CRF layer dari tfa.layers.CRF\n",
        "from tensorflow_addons.text.crf import crf_decode\n",
        "\n",
        "logits = Dense(num_ner)(x)\n",
        "# CRF param\n",
        "trans_params = tf.Variable(tf.random.uniform(shape=(num_ner, num_ner)))\n",
        "# Akan gunakan model compile custom training loop\n",
        "\n",
        "# Kita buat model tanpa compile dulu, karena CRF butuh custom train_step\n",
        "model = Model(inputs=input_layer, outputs=[intent_out, logits])"
      ],
      "metadata": {
        "id": "sXb6JkOH9c6R"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kita akan override train_step agar CRF loss bisa dihitung\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn_intent = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "# Assign the optimizer to the model\n",
        "model.compile(optimizer=optimizer, loss={'intent_output': loss_fn_intent}) # loss for intent is specified here\n",
        "\n",
        "@tf.function\n",
        "def train_step(X, Y_intent, Y_ner):\n",
        "    with tf.GradientTape() as tape:\n",
        "        intent_pred, ner_logits = model(X, training=True)\n",
        "        # Intent loss\n",
        "        loss_intent = loss_fn_intent(Y_intent, intent_pred)\n",
        "\n",
        "        # CRF loss\n",
        "        log_likelihood, _ = tfa.text.crf.crf_log_likelihood(\n",
        "            ner_logits, Y_ner, tf.reduce_sum(tf.cast(X!=0, tf.int32), axis=1), trans_params\n",
        "        )\n",
        "        loss_ner = -tf.reduce_mean(log_likelihood)\n",
        "\n",
        "        loss = loss_intent + loss_ner\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables+[trans_params])\n",
        "    model.optimizer.apply_gradients(zip(grads, model.trainable_variables+[trans_params])) # model.optimizer now refers to the Adam optimizer\n",
        "    return loss, loss_intent, loss_ner\n",
        "\n",
        "@tf.function\n",
        "def val_step(X, Y_intent, Y_ner):\n",
        "    intent_pred, ner_logits = model(X, training=False)\n",
        "    loss_intent = loss_fn_intent(Y_intent, intent_pred)\n",
        "    log_likelihood, _ = tfa.text.crf.crf_log_likelihood(\n",
        "        ner_logits, Y_ner, tf.reduce_sum(tf.cast(X!=0, tf.int32), axis=1), trans_params\n",
        "    )\n",
        "    loss_ner = -tf.reduce_mean(log_likelihood)\n",
        "    loss = loss_intent + loss_ner\n",
        "    return loss, loss_intent, loss_ner, intent_pred, ner_logits\n",
        "\n",
        "# Simple training loop\n",
        "batch_size = 16\n",
        "epochs = 10\n",
        "num_train = X_train.shape[0]\n",
        "num_val = X_val.shape[0]\n",
        "train_steps = num_train//batch_size\n",
        "val_steps = num_val//batch_size\n",
        "\n",
        "best_val_loss = np.inf\n",
        "patience = 3\n",
        "no_improve = 0\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    # Shuffle train\n",
        "    idx = np.arange(num_train)\n",
        "    np.random.shuffle(idx)\n",
        "    X_train_s = X_train[idx]\n",
        "    Y_train_intent_s = Y_train_intent[idx]\n",
        "    Y_train_ner_s = Y_train_ner[idx]\n",
        "\n",
        "    # Training\n",
        "    train_loss = 0\n",
        "    for i in range(train_steps):\n",
        "        batch_X = X_train_s[i*batch_size:(i+1)*batch_size]\n",
        "        batch_intent = Y_train_intent_s[i*batch_size:(i+1)*batch_size]\n",
        "        batch_ner = Y_train_ner_s[i*batch_size:(i+1)*batch_size]\n",
        "        loss, li, ln = train_step(batch_X, batch_intent, batch_ner)\n",
        "        train_loss += loss.numpy()\n",
        "    train_loss /= train_steps\n",
        "\n",
        "    # Validation\n",
        "    val_loss_cum = 0\n",
        "    all_intent_pred = []\n",
        "    all_intent_true = []\n",
        "    all_ner_pred = []\n",
        "    all_ner_true = []\n",
        "    for i in range(val_steps):\n",
        "        batch_X = X_val[i*batch_size:(i+1)*batch_size]\n",
        "        batch_intent = Y_val_intent[i*batch_size:(i+1)*batch_size]\n",
        "        batch_ner = Y_val_ner[i*batch_size:(i+1)*batch_size]\n",
        "        vloss, vli, vln, intent_p, ner_logits = val_step(batch_X, batch_intent, batch_ner)\n",
        "        val_loss_cum += vloss.numpy()\n",
        "        # Intent eval\n",
        "        ip = np.argmax(intent_p.numpy(), axis=1)\n",
        "        all_intent_pred.extend(ip.tolist())\n",
        "        all_intent_true.extend(batch_intent.tolist())\n",
        "        # NER decode\n",
        "        seq_len = np.sum(batch_X!=0,axis=1)\n",
        "        viterbi_paths = []\n",
        "        for j in range(batch_X.shape[0]):\n",
        "            logit_seq = ner_logits[j,:seq_len[j],:]\n",
        "            viterbi_path, _ = tfa.text.crf.viterbi_decode(logit_seq, trans_params)\n",
        "            viterbi_paths.append(viterbi_path)\n",
        "        all_ner_pred.extend(viterbi_paths)\n",
        "        all_ner_true.extend([ner[:l] for ner,l in zip(batch_ner, seq_len)])\n",
        "    val_loss = val_loss_cum/val_steps\n",
        "\n",
        "    intent_acc = np.mean(np.array(all_intent_pred) == np.array(all_intent_true))*100\n",
        "\n",
        "    # Convert ner to labels\n",
        "    pred_tag = [[ner_id2label[l] for l in seq] for seq in all_ner_pred]\n",
        "    true_tag = [[ner_id2label[l] for l in seq] for seq in all_ner_true]\n",
        "\n",
        "    print(f\"Epoch {epoch}/{epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Intent Acc: {intent_acc:.2f}%\")\n",
        "    print(seq_classification_report(true_tag, pred_tag))\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        no_improve = 0\n",
        "        # Simpan model weights\n",
        "        model.save_weights('best_multitask_model.h5')\n",
        "        np.save('best_trans_params.npy', trans_params.numpy())\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early Stopping...\")\n",
        "            break\n",
        "\n",
        "# Load best weights\n",
        "model.load_weights('best_multitask_model.h5')\n",
        "trans_params.assign(np.load('best_trans_params.npy'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQWdF96K-A30",
        "outputId": "a5bb150d-9767-4af4-ecc5-c0aa0c002aae"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 10.4508, Val Loss: 9.1661, Intent Acc: 56.25%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       0.00      0.00      0.00        10\n",
            "   condition       0.00      0.00      0.00         2\n",
            "    location       0.00      0.00      0.00         3\n",
            "      status       0.00      0.00      0.00         1\n",
            "     symptom       0.17      0.33      0.23        18\n",
            "\n",
            "   micro avg       0.17      0.18      0.17        34\n",
            "   macro avg       0.03      0.07      0.05        34\n",
            "weighted avg       0.09      0.18      0.12        34\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10\n",
            "Train Loss: 8.2152, Val Loss: 7.3527, Intent Acc: 56.25%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       0.48      1.00      0.65        10\n",
            "   condition       0.00      0.00      0.00         2\n",
            "    location       0.00      0.00      0.00         3\n",
            "      status       0.00      0.00      0.00         1\n",
            "     symptom       0.26      0.39      0.31        18\n",
            "\n",
            "   micro avg       0.35      0.50      0.41        34\n",
            "   macro avg       0.15      0.28      0.19        34\n",
            "weighted avg       0.28      0.50      0.35        34\n",
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 6.9263, Val Loss: 6.6060, Intent Acc: 50.00%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       1.00      0.60      0.75        10\n",
            "   condition       0.00      0.00      0.00         2\n",
            "    location       0.33      0.33      0.33         3\n",
            "      status       0.00      0.00      0.00         1\n",
            "     symptom       0.26      0.28      0.27        18\n",
            "\n",
            "   micro avg       0.43      0.35      0.39        34\n",
            "   macro avg       0.32      0.24      0.27        34\n",
            "weighted avg       0.46      0.35      0.39        34\n",
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 5.7945, Val Loss: 5.6040, Intent Acc: 50.00%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       0.67      1.00      0.80        10\n",
            "   condition       0.00      0.00      0.00         2\n",
            "    location       0.00      0.00      0.00         3\n",
            "      status       0.00      0.00      0.00         1\n",
            "     symptom       0.29      0.39      0.33        18\n",
            "\n",
            "   micro avg       0.42      0.50      0.46        34\n",
            "   macro avg       0.19      0.28      0.23        34\n",
            "weighted avg       0.35      0.50      0.41        34\n",
            "\n",
            "Epoch 5/10\n",
            "Train Loss: 4.8754, Val Loss: 5.1145, Intent Acc: 68.75%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       0.67      0.80      0.73        10\n",
            "   condition       0.00      0.00      0.00         2\n",
            "    location       1.00      0.33      0.50         3\n",
            "      status       0.00      0.00      0.00         1\n",
            "     symptom       0.35      0.33      0.34        18\n",
            "\n",
            "   micro avg       0.50      0.44      0.47        34\n",
            "   macro avg       0.40      0.29      0.31        34\n",
            "weighted avg       0.47      0.44      0.44        34\n",
            "\n",
            "Epoch 6/10\n",
            "Train Loss: 4.2395, Val Loss: 4.3231, Intent Acc: 75.00%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       0.77      1.00      0.87        10\n",
            "   condition       0.00      0.00      0.00         2\n",
            "    location       0.33      0.67      0.44         3\n",
            "      status       0.00      0.00      0.00         1\n",
            "     symptom       0.44      0.39      0.41        18\n",
            "\n",
            "   micro avg       0.54      0.56      0.55        34\n",
            "   macro avg       0.31      0.41      0.35        34\n",
            "weighted avg       0.49      0.56      0.51        34\n",
            "\n",
            "Epoch 7/10\n",
            "Train Loss: 3.7018, Val Loss: 4.1055, Intent Acc: 68.75%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       0.83      1.00      0.91        10\n",
            "   condition       0.00      0.00      0.00         2\n",
            "    location       0.50      0.67      0.57         3\n",
            "      status       0.00      0.00      0.00         1\n",
            "     symptom       0.33      0.39      0.36        18\n",
            "\n",
            "   micro avg       0.51      0.56      0.54        34\n",
            "   macro avg       0.33      0.41      0.37        34\n",
            "weighted avg       0.47      0.56      0.51        34\n",
            "\n",
            "Epoch 8/10\n",
            "Train Loss: 3.1816, Val Loss: 3.8687, Intent Acc: 62.50%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       1.00      0.90      0.95        10\n",
            "   condition       0.00      0.00      0.00         2\n",
            "    location       0.40      0.67      0.50         3\n",
            "      status       0.00      0.00      0.00         1\n",
            "     symptom       0.40      0.44      0.42        18\n",
            "\n",
            "   micro avg       0.56      0.56      0.56        34\n",
            "   macro avg       0.36      0.40      0.37        34\n",
            "weighted avg       0.54      0.56      0.55        34\n",
            "\n",
            "Epoch 9/10\n",
            "Train Loss: 2.7563, Val Loss: 3.7346, Intent Acc: 62.50%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       1.00      1.00      1.00        10\n",
            "   condition       1.00      0.50      0.67         2\n",
            "    location       0.00      0.00      0.00         3\n",
            "      status       0.00      0.00      0.00         1\n",
            "     symptom       0.37      0.39      0.38        18\n",
            "\n",
            "   micro avg       0.56      0.53      0.55        34\n",
            "   macro avg       0.47      0.38      0.41        34\n",
            "weighted avg       0.55      0.53      0.53        34\n",
            "\n",
            "Epoch 10/10\n",
            "Train Loss: 2.6272, Val Loss: 3.5241, Intent Acc: 75.00%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       1.00      1.00      1.00        10\n",
            "   condition       1.00      0.50      0.67         2\n",
            "    location       0.00      0.00      0.00         3\n",
            "      status       0.00      0.00      0.00         1\n",
            "     symptom       0.39      0.39      0.39        18\n",
            "\n",
            "   micro avg       0.58      0.53      0.55        34\n",
            "   macro avg       0.48      0.38      0.41        34\n",
            "weighted avg       0.56      0.53      0.54        34\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'UnreadVariable' shape=(11, 11) dtype=float32, numpy=\n",
              "array([[ 0.6593117 ,  0.43100312, -0.00389422,  0.07282911,  0.81275475,\n",
              "         0.24689274,  0.12578773,  0.6911004 ,  0.85904115,  0.5607759 ,\n",
              "         0.24619196],\n",
              "       [ 0.74130625,  0.25703546,  0.23628859,  0.6759545 ,  0.68496174,\n",
              "         0.6025608 ,  0.3355796 , -0.00960143,  0.90292907, -0.0031366 ,\n",
              "         0.16572301],\n",
              "       [ 0.09484031,  0.23557076,  0.39058518,  0.25497198,  0.24212798,\n",
              "         0.58641374,  0.189031  ,  0.93358505,  0.04939082,  0.14150749,\n",
              "        -0.01871487],\n",
              "       [-0.01139066,  0.9482258 ,  0.19912736,  0.38568377,  0.7095453 ,\n",
              "         0.33143774,  0.3341312 ,  0.27803013,  0.750947  ,  0.73537177,\n",
              "         0.3380729 ],\n",
              "       [ 0.2609268 ,  0.25051278,  0.41061822,  0.4529809 ,  0.83135384,\n",
              "         0.8879625 ,  0.32480913,  0.31689855,  0.3299686 ,  0.6131312 ,\n",
              "         0.91074073],\n",
              "       [ 0.8167935 ,  0.28066316,  0.9565467 ,  0.5284629 ,  0.4354476 ,\n",
              "         0.0733595 ,  0.3864483 ,  0.9185173 ,  0.78783756,  0.10133728,\n",
              "         0.12001574],\n",
              "       [ 0.04990712,  0.89821404,  0.7383454 ,  0.38717777,  0.4563057 ,\n",
              "         0.22680295,  0.5505984 ,  0.36738753,  0.7009203 ,  0.560279  ,\n",
              "         0.87413454],\n",
              "       [ 0.8655576 ,  0.8978875 ,  0.76441246,  0.03852701,  0.6256998 ,\n",
              "         0.83620656,  0.4472406 ,  0.84009254,  0.13081804,  0.8686647 ,\n",
              "         0.5774717 ],\n",
              "       [ 0.04647777,  0.21156988,  0.46859407,  0.96156675,  0.6528153 ,\n",
              "         0.29079726,  0.28613827,  0.46624497,  0.7979457 ,  0.08328374,\n",
              "         0.12901217],\n",
              "       [ 0.5376233 ,  0.8036992 ,  0.70130074,  0.7469446 ,  0.15070285,\n",
              "         0.15284158,  0.6988469 ,  0.11335341,  0.39274088,  0.68226504,\n",
              "         0.40752265],\n",
              "       [ 0.05585111,  0.254929  ,  0.29795048,  0.5365652 ,  0.3410324 ,\n",
              "         0.19695888,  0.87003976,  0.65473235,  0.9616884 ,  0.6903454 ,\n",
              "         0.5490951 ]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DialogManager:\n",
        "    def __init__(self):\n",
        "        self.state = 'IDLE'\n",
        "        self.reported_animal = None\n",
        "        self.symptoms = []\n",
        "        self.consultation_recommended = False\n",
        "\n",
        "    def predict_intent_ner(self, user_input):\n",
        "        user_clean = preprocess_text(user_input)\n",
        "        seq = tokenizer.texts_to_sequences([user_clean])\n",
        "        seq_padded = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "        intent_p, ner_logits = model(seq_padded, training=False)\n",
        "        intent_id = np.argmax(intent_p.numpy())\n",
        "        intent = intent_id2label[intent_id]\n",
        "\n",
        "        length = np.sum(seq_padded!=0, axis=1)[0]\n",
        "        logit_seq = ner_logits[0,:length,:]\n",
        "        viterbi_path, _ = tfa.text.crf.viterbi_decode(logit_seq, trans_params)\n",
        "        entities = []\n",
        "        tokens = user_clean.split()\n",
        "        for i, l_id in enumerate(viterbi_path):\n",
        "            lbl = ner_id2label[l_id]\n",
        "            if lbl != 'O':\n",
        "                ent_type = lbl.split('-')[1]\n",
        "                entities.append({'entity': ent_type, 'value': tokens[i]})\n",
        "        return intent, entities\n",
        "\n",
        "    def get_response(self, user_input):\n",
        "        intent, entities = self.predict_intent_ner(user_input)\n",
        "\n",
        "        # Contoh logika multi-turn sederhana\n",
        "        if intent == \"Greeting\":\n",
        "            return \"Halo! Ada yang bisa saya bantu hari ini?\"\n",
        "        elif intent == \"Thanks\":\n",
        "            return \"Sama-sama! Senang membantu.\"\n",
        "        elif intent == \"Rekomendasi Penanganan Awal\":\n",
        "            # Periksa entitas\n",
        "            animal_ents = [e for e in entities if e['entity']=='animal']\n",
        "            symptom_ents = [e for e in entities if e['entity']=='symptom']\n",
        "            if animal_ents:\n",
        "                self.reported_animal = animal_ents[0]['value']\n",
        "            if symptom_ents:\n",
        "                self.symptoms.extend(s['value'] for s in symptom_ents)\n",
        "\n",
        "            # Jika banyak gejala, mungkin rekomendasikan dokter\n",
        "            if len(self.symptoms) > 2 and not self.consultation_recommended:\n",
        "                self.consultation_recommended = True\n",
        "                return f\"Saya mencatat {self.reported_animal if self.reported_animal else 'hewan'} dengan gejala {', '.join(self.symptoms)}. \" \\\n",
        "                       \"Kondisi tampak serius. Sebaiknya konsultasikan dengan dokter hewan.\"\n",
        "\n",
        "            return f\"Saya mencatat {self.reported_animal if self.reported_animal else 'hewan'} dengan gejala {', '.join(self.symptoms)}. \" \\\n",
        "                   \"Pastikan kondisi lingkungan nyaman dan jika tidak membaik, hubungi dokter hewan.\"\n",
        "        else:\n",
        "            return \"Maaf, saya belum mengerti maksud Anda.\"\n",
        "\n",
        "dm = DialogManager()\n",
        "print(dm.get_response(\"Halo, saya punya pertanyaan tentang anjing saya.\"))\n",
        "print(dm.get_response(\"Anjing saya muntah dan diare sejak kemarin.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhDZt8dn-qcM",
        "outputId": "2591ff40-e1c8-4b03-b188-351ff9e75880"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saya mencatat hewan dengan gejala . Pastikan kondisi lingkungan nyaman dan jika tidak membaik, hubungi dokter hewan.\n",
            "Maaf, saya belum mengerti maksud Anda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dm.get_response(\"Kucing saya terlihat lesu dan kehilangan nafsu makan. Apa yang harus saya lakukan?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV6fAQBi_D0k",
        "outputId": "21314489-cd03-4a34-c816-17e6d7aba426"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saya mencatat kucing dengan gejala lesu, hilang, nafsu, makan. Kondisi tampak serius. Sebaiknya konsultasikan dengan dokter hewan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dm.get_response(\"Anjing saya terlihat pincang setelah bermain. Apa langkah awal yang harus dilakukan?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBQqCJHD_ZZ0",
        "outputId": "47542188-47f4-440a-8108-55950a55d23c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saya mencatat anjing dengan gejala lesu, hilang, nafsu, makan, pincang. Pastikan kondisi lingkungan nyaman dan jika tidak membaik, hubungi dokter hewan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lssy1dvh_vf3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}